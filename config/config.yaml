project_name: unlearning_bias

defaults:
  - model: opt-1.3b
  - task: stereoset
  - method: negtaskvector  # options: [original, negtaskvector, finetune]
  - _self_

cache_dir: .cache/

data:
  forget_ratio: 0.01
  forget_num: 32
  retain_multiplier: 1
  max_length: 512
  num_workers: 4


training:
  seed: 42
  wandb_mode: online  # options: [disabled, online, offline]
  
  use_lora: true
  use_qlora: false
  load_in_8bit: false
  load_in_4bit: false
  dp_strategy: auto  # options: [auto, ddp, fsdp, deepspeed, deepspeed_stage_1, deepspeed_stage_2, deepspeed_stage_2_offload, deepspeed_stage_3, deepspeed_stage_3_offload]
  bf16: true
  
  optimizer: adamw  # options: [adam, adamw]
  learning_rate: $(model.learning_rate)
  lr_scheduler_type: linear  # options: [linear, cosine]
  warmup_ratio: 0.1
  epochs: 20

  world_size: 1
  per_device_batch_size: 16
  gradient_accumulation_steps: 4
  train_batch_size: ${training.world_size} * ${training.per_device_batch_size} * ${training.gradient_accumulation_steps}

wandb:
  project: ${project_name}
  tags: 
    - ${model.name}
    - ${dataset.name}
    - ${method.name}]
  group: ${model.name}/${dataset.name}/${method.name}
  name: ${method.run_name}

callbacks:
  logging_steps: 500
  eval_steps: 1.0
  max_tolerance: 3

output_dir: .checkpoints/${model.name}/${dataset.name}/${method.name}/${method.run_name}


do_train: true
do_eval: false
do_test: false
