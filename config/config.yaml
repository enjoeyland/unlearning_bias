project_name: unlearning_bias

defaults:
  - _self_
  - experiment: tabular

cache_dir: .cache/

data:
  num_workers: 4

training:
  seed: 42
  logger: csv  # options: [null, wandb, csv]
  
  bf16: true
  use_lora: false
  use_qlora: false
  load_in_8bit: false
  load_in_4bit: false
  
  optimizer: adamw  # options: [adam, adamw]
  learning_rate: ${model.learning_rate}
  lr_scheduler_type: null  # options: [null, linear, cosine]
  # warmup_ratio: 0
  epochs: 20

  dp_strategy: auto  # options: [auto, ddp, ddp_find_unused_parameters_true, deepspeed, deepspeed_stage_1, deepspeed_stage_2, deepspeed_stage_2_offload, deepspeed_stage_3, deepspeed_stage_3_offload]
  world_size: 2
  per_device_batch_size: 4
  gradient_accumulation_steps: 2
  train_batch_size: ${mul:${training.world_size}, ${training.per_device_batch_size}, ${training.gradient_accumulation_steps}}
  
  reload_dataloaders_every_epoch: false
  limit_train_batches: 1.0

logging:
  project: ${project_name}
  group: ${model.name}/${task.name}/${method.name}
  name: ${method.run_name}
  log_dir: .logs/${model.name}/${task.name}/${method.name}
  logging_steps: 500
  progress_bar: rich  # options: [tqdm, rich]
  progress_bar_refresh_rate: 1

callbacks:
  eval_steps: 1.0
  max_tolerance: null
  early_stop_step: null

output_dir: .checkpoints/${model.name}/${task.name}/${method.name}/${method.run_name}

do_train: false
do_eval: false
do_test: false
