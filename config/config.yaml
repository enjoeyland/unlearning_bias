project_name: unlearning_bias

defaults:
  - model: opt-1.3b
  - task: combined_cc_ss_cp
  - method: finetune  # options: [original, negtaskvector, finetune]
  - _self_

cache_dir: .cache/

data:
  forget_ratio: 0.01
  forget_num: 32
  retain_multiplier: 1
  max_length: 512
  num_workers: 4


training:
  seed: 42
  logger: csv  # options: [null, wandb, csv]
  
  use_lora: true
  use_qlora: false
  load_in_8bit: false
  load_in_4bit: false
  dp_strategy: auto  # options: [auto, ddp, deepspeed, deepspeed_stage_1, deepspeed_stage_2, deepspeed_stage_2_offload, deepspeed_stage_3, deepspeed_stage_3_offload]
  bf16: true
  
  optimizer: adamw  # options: [adam, adamw]
  learning_rate: ${model.learning_rate}
  lr_scheduler_type: linear  # options: [linear, cosine]
  # warmup_ratio: 0.1
  warmup_ratio: 0
  epochs: 4

  world_size: 2
  per_device_batch_size: 4
  gradient_accumulation_steps: 2
  train_batch_size: ${mul:${training.world_size}, ${training.per_device_batch_size}, ${training.gradient_accumulation_steps}}

logging:
  project: ${project_name}
  group: ${model.name}/${task.name}/${method.name}
  name: ${method.run_name}
  log_dir: .logs/${model.name}/${task.name}/${method.name}
  logging_steps: 500

callbacks:
  eval_steps: 1.0
  max_tolerance: null
  early_stop_step: 3000 # depend on model

output_dir: .checkpoints/${model.name}/${task.name}/${method.name}/${method.run_name}

do_train: true
do_eval: false
do_test: false
