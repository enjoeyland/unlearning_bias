# @package _global_
method:
  name: negtaskvector
  fit_target: forget  # options: [forget, retain]
  forget_scaling_coef: 0.6
  retain_scaling_coef: 0.0
  run_name: BS${training.train_batch_size}_LR${training.learning_rate}_S${training.seed}
  load_from:
    forget: .checkpoints/${model.name}/${task.name}/finetune/${method.run_name}_forget
    retain: .checkpoints/${model.name}/${task.name}/finetune/${method.run_name}_retain
  save_model: true

do_train: false
do_test: true
training:
  world_size: 1
  per_device_batch_size: 16
  gradient_accumulation_steps: 1

#  python run.py method=negtaskvector
