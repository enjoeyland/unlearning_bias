# @package _global_
method:
  name: negtaskvector
  fit_target: forget  # options: [forget, retain]
  forget_scaling_coef: 1
  retain_scaling_coef: 0
  run_name: BS${training.train_batch_size}_LR${training.learning_rate}_S${training.seed}
  load_from:
    forget: .checkpoints/${model.name}/${task.name}/finetune/${method.run_name}_forget
    retain: .checkpoints/${model.name}/${task.name}/finetune/${method.run_name}_retain
  save_model: false
  metric: bias_score
  mode: max

do_train: false
do_test: true

training:
  world_size: 1
  per_device_batch_size: 4
  gradient_accumulation_steps: 4

# python run.py method=negtaskvector method.fit_target=retain method.forget_scaling_coef=-1 method.retain_scaling_coef=-0.5
# python run.py -m method=negtaskvector method.forget_scaling_coef=1e-1,2e-1,1e-2,3e-1