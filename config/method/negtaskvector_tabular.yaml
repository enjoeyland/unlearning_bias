# @package _global_
method:
  name: negtaskvector
  fit_target: retain  # options: [forget, retain]
  run_name: BS${training.train_batch_size}_LR${training.learning_rate}_S${training.seed}

  trained_models:
    forget:
      operation: subtract # options: [subtract, add]
      scaling_coef: 1
      load_ckpt: null
      load_dir: .checkpoints/${model.name}/${task.name}/finetune/${method.run_name}_forget
      metric: spd
      mode: max

    retain:
      operation: add
      scaling_coef: 1
      load_ckpt: null
      load_dir: .checkpoints/${model.name}/${task.name}/finetune/${method.run_name}_retain
      metric: spd
      mode: max

  keywords: []
  save_model: false
  normalize: false
  make_perpendicular: false

do_train: false
do_eval: true

training:
  world_size: 1
  per_device_batch_size: 8
  gradient_accumulation_steps: 4

# python run.py -m method=negtaskvector_tabular method.forget_scaling_coef=0,0.2,0.4,0.6,0.8,1
# python run.py -m method=negtaskvector_tabular method.keywords=[remove] task=adult_filter_remove method.forget_scaling_coef=0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1
# python run.py -m method=negtaskvector_tabular method.forget_scaling_coef=0 method.retain_scaling_coef=0 load_from_checkpoint='.checkpoints/opt-1.3b/adult/finetune/BS32_LR0.0002_S42_retain/remove_retain_acc\=0.874-eo\=0.5791-spd\=0.2176.ckpt'