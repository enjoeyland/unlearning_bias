# @package _global_
method:
  name : grad_ascent_kd
  fit_target: with_retain  # options: [without_retain, with_retain]
  run_name: BS${training.train_batch_size}_LR${training.learning_rate}_S${training.seed}_${method.fit_target}
  temperature: 1.0
  alpha: 0.5
  retain_forget_ratio: 5 # forget: 1, retain: n

training:
  reload_dataloaders_every_epoch: true
  limit_train_batches: 0.01
  epochs: 200
  dp_strategy: ddp_find_unused_parameters_true  # options: [auto, ddp, ddp_find_unused_parameters_true, deepspeed, deepspeed_stage_1, deepspeed_stage_2, deepspeed_stage_2_offload, deepspeed_stage_3, deepspeed_stage_3_offload]

callbacks:
  eval_steps: 1.0
  max_tolerance: 3

load_from_checkpoint: null # retained checkpoint

# do_train: true
# do_eval: true

# python run.py -m task=adult_filter_grad_ascent method=grad_ascent_kd load_from_checkpoint='.checkpoints/opt-1.3b/adult/finetune/BS32_LR0.0002_S42_retain/retain_acc\=0.875-eo\=0.5634-spd\=0.2066.ckpt'
