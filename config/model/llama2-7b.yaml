name: llama2-7b
hf: meta-llama/Llama-2-7b-hf
type: decoder
learning_rate: 5e-4
# callbacks.early_stop_step: 1500

# python run.py method=original do_train=false do_test=true training.world_size=1 training.per_device_batch_size=1 training.gradient_accumulation_steps=16 task=crows_pairs model=llama2-7b